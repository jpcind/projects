#-------------------------------------------------------------------------
# AUTHOR: Joey Cindass
# FILENAME: jcin_bagging_random_forest.py
# SPECIFICATION: comparing performance of decision tree, ensemble classifier, random forest
# FOR: CS 4210- Assignment #4
# TIME SPENT: 4 hours
#-----------------------------------------------------------*/

#IMPORTANT NOTE: DO NOT USE ANY ADVANCED PYTHON LIBRARY TO COMPLETE THIS CODE SUCH AS numpy OR pandas. You have to work here only with standard vectors and arrays

#importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier

dbTraining = []
dbTest = []
X_training = []
y_training = []
classVotes = [] #this array will be used to count the votes of each classifier

#reading the training data from a csv file and populate dbTraining
#--> add your Python code here
open_training = open('optdigits.tra', 'r')
for mylist in open_training:
    mylist = mylist.strip().split(sep=',')
    mylist = list(map(lambda x: int(x), mylist))
    dbTraining.append(mylist)


# #reading the test data from a csv file and populate dbTest
# #--> add your Python code here
open_test = open('optdigits.tes', 'r')
for mylist in open_test:
    mylist = mylist.strip().split(sep=',')
    mylist = list(map(lambda x: int(x), mylist))
    dbTest.append(mylist)
# print(len(dbTest[-1]))

# #inititalizing the class votes for each test sample. Example: classVotes.append([0,0,0,0,0,0,0,0,0,0])
# #--> add your Python code here
initialize = []
for _ in range(10):
    initialize.append(0)
for _ in range(len(dbTest)):
    classVotes.append(initialize)

# print(classVotes)
print("Started my base and ensemble classifier ...")

# we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample
for k in range(20):
    bootstrapSample = resample(dbTraining, n_samples=len(dbTraining), replace=True)




#   #populate the values of X_training and y_training by using the bootstrapSample
    for sample in bootstrapSample:
        X_training.append(sample[:64])
        y_training.append(sample[-1])

#   #fitting the decision tree to the data
    # we will use a single decision tree without pruning it
    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)
    clf = clf.fit(X_training, y_training)
    for i, testSample in enumerate(dbTest):




        # make the classifier prediction for each test sample and update
        # the corresponding index value in classVotes.
        # For instance,
        # if your first base classifier predicted 2 for the first test sample,
        # then classVotes[0,0,0,0,0,0,0,0,0,0] will change to classVotes[0,0,1,0,0,0,0,0,0,0].
        # Later, if your second base classifier predicted 3 for the first test sample,
        # then classVotes[0,0,1,0,0,0,0,0,0,0] will change to classVotes[0,0,1,1,0,0,0,0,0,0]
        # Later, if your third base classifier predicted 3 for the first test sample,
        # then classVotes[0,0,1,1,0,0,0,0,0,0] will change to classVotes[0,0,1,2,0,0,0,0,0,0]
        # this array will consolidate the votes of all classifier for all test samples
        pred = clf.predict([testSample[:64]])[0]
        classVotes[i][pred] += 1


#
        # if k == 0: #for only the first base classifier, compare the prediction with the true label of the test
        # sample here to start calculating its accuracy #--> add your Python code here
        correct = 0
        if k == 0 and pred == testSample[-1]:
            correct += 1
    # for only the first base classifier, print its accuracy here
    if k == 0:
        accuracy = correct / len(dbTest)


        print("Finished my base classifier (fast but relatively low accuracy) ...")
        print("My base classifier accuracy: " + str(accuracy))
        print("")


# #now, compare the final ensemble prediction (majority vote in classVotes) for each test sample with the ground
# truth label to calculate the accuracy of the ensemble classifier (all base classifiers together)

ensemble_correct = 0
for i, testSample in enumerate(dbTest):
    x = classVotes[i].index(max(classVotes[i]))
    y = classVotes[i]
    if x == testSample[-1]:
        ensemble_correct += 1

ensemble_accuracy = ensemble_correct / len(dbTest)

# #printing the ensemble accuracy here
print("Finished my ensemble classifier (slow but higher accuracy) ...")
print("My ensemble accuracy: " + str(ensemble_accuracy))
print("")

print("Started Random Forest algorithm ...")

# #Create a Random Forest Classifier
# this is the number of decision trees that will be generated by Random
# Forest. The sample of the ensemble method used before
clf = RandomForestClassifier(n_estimators=20)

# Fit Random Forest to the training data
clf.fit(X_training,y_training)

# make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
rf_correct = 0
for i in dbTest:
    my_testSample = testSample[:64]
    pred = clf.predict([my_testSample])[0]


# compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
    if pred == testSample[-1]:
        rf_correct += 1
rf_acc = rf_correct / len(dbTest)

# printing Random Forest accuracy here
print("Random Forest accuracy: " + str(rf_acc))

print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
